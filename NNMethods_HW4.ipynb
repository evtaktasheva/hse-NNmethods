{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers==4.12.2\n!pip install -q datasets","metadata":{"id":"Jjsbi1u3QFEM","outputId":"a15b6363-a1c2-437f-b78d-4c9c4ef3d77e","execution":{"iopub.status.busy":"2022-03-14T14:39:03.006600Z","iopub.execute_input":"2022-03-14T14:39:03.006933Z","iopub.status.idle":"2022-03-14T14:39:25.146567Z","shell.execute_reply.started":"2022-03-14T14:39:03.006856Z","shell.execute_reply":"2022-03-14T14:39:25.145775Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import transformers\nfrom transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n\nfrom datasets import load_dataset\nfrom functools import partial\nfrom transformers import Trainer, TrainingArguments, DataCollatorWithPadding\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\n\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom textwrap import wrap\nfrom torch import nn\nimport os","metadata":{"id":"Z1jvVHb6CaW-","execution":{"iopub.status.busy":"2022-03-14T15:22:05.309547Z","iopub.execute_input":"2022-03-14T15:22:05.310360Z","iopub.status.idle":"2022-03-14T15:22:05.316844Z","shell.execute_reply.started":"2022-03-14T15:22:05.310312Z","shell.execute_reply":"2022-03-14T15:22:05.316120Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"id":"U2VNiOyAkHYX","execution":{"iopub.status.busy":"2022-03-14T14:42:09.060938Z","iopub.execute_input":"2022-03-14T14:42:09.061517Z","iopub.status.idle":"2022-03-14T14:42:09.065789Z","shell.execute_reply.started":"2022-03-14T14:42:09.061477Z","shell.execute_reply":"2022-03-14T14:42:09.064594Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n%config InlineBackend.figure_format='retina'\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\nrcParams['figure.figsize'] = 8, 6","metadata":{"id":"co9bB2BPWiA6","execution":{"iopub.status.busy":"2022-03-14T14:42:09.300201Z","iopub.execute_input":"2022-03-14T14:42:09.300399Z","iopub.status.idle":"2022-03-14T14:42:09.320711Z","shell.execute_reply.started":"2022-03-14T14:42:09.300374Z","shell.execute_reply":"2022-03-14T14:42:09.319940Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Загрузка данных\n\nСразу будем загружать не все данные, а только часть, чтобы ускорить процесс обучения и пожалеть бедный колаб","metadata":{"id":"A8522g7JIu5J"}},{"cell_type":"code","source":"raw_datasets = load_dataset('imdb', split={'train': 'train[:1500]+train[-1500:]', 'test': 'test[:500]+test[-500:]', 'validation': 'test[500:1000]+test[-1000:-500]'})","metadata":{"id":"7P8PRBML_Idu","outputId":"dcffd0aa-1678-49f6-dd50-dda3565ca60f","execution":{"iopub.status.busy":"2022-03-14T14:42:09.706637Z","iopub.execute_input":"2022-03-14T14:42:09.707147Z","iopub.status.idle":"2022-03-14T14:42:10.027665Z","shell.execute_reply.started":"2022-03-14T14:42:09.707117Z","shell.execute_reply":"2022-03-14T14:42:10.027024Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c0b65f320774ab7b63b5aa26cf98f8a"}},"metadata":{}}]},{"cell_type":"code","source":"raw_datasets","metadata":{"id":"KWtegITPQ1WQ","outputId":"321d328c-3bc2-4bd6-caec-646fa103af32","execution":{"iopub.status.busy":"2022-03-14T14:42:10.029289Z","iopub.execute_input":"2022-03-14T14:42:10.029745Z","iopub.status.idle":"2022-03-14T14:42:10.035044Z","shell.execute_reply.started":"2022-03-14T14:42:10.029694Z","shell.execute_reply":"2022-03-14T14:42:10.034401Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 3000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 1000\n    })\n    validation: Dataset({\n        features: ['text', 'label'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Токенизируем данные","metadata":{"id":"fz7LBabQEKRr"}},{"cell_type":"code","source":"MODEL_NAME = 'bert-base-multilingual-cased'\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)","metadata":{"id":"bbaWRxWiX_X-","outputId":"4ff4ceaf-ba38-4fca-c6f4-b827505f63b0","execution":{"iopub.status.busy":"2022-03-14T14:42:10.450972Z","iopub.execute_input":"2022-03-14T14:42:10.451451Z","iopub.status.idle":"2022-03-14T14:42:11.603372Z","shell.execute_reply.started":"2022-03-14T14:42:10.451419Z","shell.execute_reply":"2022-03-14T14:42:11.602609Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"directionality\": \"bidi\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.12.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 119547\n}\n\nloading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\nloading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\nloading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\nloading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"directionality\": \"bidi\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.12.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 119547\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_function(examples, tokenizer):\n    result = tokenizer(examples[\"text\"], max_length=256, padding='max_length', truncation=True)\n    result[\"label\"] = examples[\"label\"]\n    return result","metadata":{"id":"aqVwDzJbSySP","execution":{"iopub.status.busy":"2022-03-14T14:42:11.605103Z","iopub.execute_input":"2022-03-14T14:42:11.605371Z","iopub.status.idle":"2022-03-14T14:42:11.610173Z","shell.execute_reply.started":"2022-03-14T14:42:11.605333Z","shell.execute_reply":"2022-03-14T14:42:11.609359Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = raw_datasets.map(\n    partial(preprocess_function, tokenizer=tokenizer),\n    batched=True,\n    desc=\"Running tokenizer on dataset\"\n)","metadata":{"id":"sC7QNmv9TGFh","outputId":"adce46f7-b1de-4639-a8f7-cf93b9f39374","execution":{"iopub.status.busy":"2022-03-14T14:42:11.611472Z","iopub.execute_input":"2022-03-14T14:42:11.611756Z","iopub.status.idle":"2022-03-14T14:42:12.661078Z","shell.execute_reply.started":"2022-03-14T14:42:11.611700Z","shell.execute_reply":"2022-03-14T14:42:12.660101Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7ae1cc3277948538cbf703a861a098c"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets","metadata":{"id":"538uyfrkVteQ","outputId":"a7cfbfa4-41fc-418a-98ad-fffd21b142d3","execution":{"iopub.status.busy":"2022-03-14T14:42:12.663170Z","iopub.execute_input":"2022-03-14T14:42:12.663420Z","iopub.status.idle":"2022-03-14T14:42:12.672542Z","shell.execute_reply.started":"2022-03-14T14:42:12.663384Z","shell.execute_reply":"2022-03-14T14:42:12.671637Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 3000\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n    validation: Dataset({\n        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"num_labels = raw_datasets['train'].to_pandas()['label'].nunique()\nnum_labels","metadata":{"id":"9eefNAQKViIS","outputId":"efc0d7ea-6b19-4dc7-8483-52e4af566c57","execution":{"iopub.status.busy":"2022-03-14T14:42:12.673906Z","iopub.execute_input":"2022-03-14T14:42:12.674224Z","iopub.status.idle":"2022-03-14T14:42:12.699368Z","shell.execute_reply.started":"2022-03-14T14:42:12.674189Z","shell.execute_reply":"2022-03-14T14:42:12.698785Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)","metadata":{"id":"OCy1DtFMV0m9","execution":{"iopub.status.busy":"2022-03-14T14:42:12.702953Z","iopub.execute_input":"2022-03-14T14:42:12.704871Z","iopub.status.idle":"2022-03-14T14:42:12.709910Z","shell.execute_reply.started":"2022-03-14T14:42:12.704830Z","shell.execute_reply":"2022-03-14T14:42:12.709294Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"Опишем функцию для подсчета метрик, которые хотим увидеть при оценке модели:","metadata":{"id":"EUHCyMAEEhpz"}},{"cell_type":"code","source":"def compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }","metadata":{"id":"lN1WaQpV4j1c","execution":{"iopub.status.busy":"2022-03-14T14:42:12.714141Z","iopub.execute_input":"2022-03-14T14:42:12.716272Z","iopub.status.idle":"2022-03-14T14:42:12.723577Z","shell.execute_reply.started":"2022-03-14T14:42:12.714534Z","shell.execute_reply":"2022-03-14T14:42:12.722949Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## Модель 1: SentimentClassifier\n\nПростой классификатор навещенный поферх BERT'а как с семинара, но с небольшим изменением для того, чтобы пихнуть в Trainer","metadata":{"id":"_uCNi-TwXHSX"}},{"cell_type":"code","source":"class SentimentClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super().__init__()\n        self.n_classes = n_classes\n        self.bert = AutoModel.from_pretrained(MODEL_NAME)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n  \n    def forward(self, input_ids, attention_mask, labels=None):\n        last_hidden_state, pooled_output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=False)\n       \n        logits = self.out(self.drop(pooled_output))\n\n        loss = None\n        if labels is not None:\n            loss_fn = nn.CrossEntropyLoss()\n            loss = loss_fn(logits.view(-1, self.n_classes), labels.view(-1))\n\n        output = (logits,)\n        return ((loss,) + output) if loss is not None else output","metadata":{"id":"qpoV392IW9os","execution":{"iopub.status.busy":"2022-03-14T14:42:12.727609Z","iopub.execute_input":"2022-03-14T14:42:12.729015Z","iopub.status.idle":"2022-03-14T14:42:12.739856Z","shell.execute_reply.started":"2022-03-14T14:42:12.728972Z","shell.execute_reply":"2022-03-14T14:42:12.738896Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"model = SentimentClassifier(num_labels)","metadata":{"id":"BiFFS-90XUxq","outputId":"4cad834a-72a3-4f99-dc11-87ccbdb10960","execution":{"iopub.status.busy":"2022-03-14T14:42:12.822346Z","iopub.execute_input":"2022-03-14T14:42:12.822998Z","iopub.status.idle":"2022-03-14T14:42:15.246715Z","shell.execute_reply.started":"2022-03-14T14:42:12.822962Z","shell.execute_reply":"2022-03-14T14:42:15.246039Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"directionality\": \"bidi\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.12.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 119547\n}\n\nloading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\nSome weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of BertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"# заморозим некоторые слои\nfreeze_layers = list(range(5))\nfor layer_id in freeze_layers:\n    for param in list(model.bert.encoder.layer[layer_id].parameters()):\n        param.requires_grad = False","metadata":{"id":"2wqs1pdHXhx8","execution":{"iopub.status.busy":"2022-03-14T14:42:15.249624Z","iopub.execute_input":"2022-03-14T14:42:15.249977Z","iopub.status.idle":"2022-03-14T14:42:15.255332Z","shell.execute_reply.started":"2022-03-14T14:42:15.249946Z","shell.execute_reply":"2022-03-14T14:42:15.254667Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"Зададим параметры обучения с помощью TrainingArguments:","metadata":{"id":"AecmyljLE0hU"}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results/sentiment-classifier',\n    overwrite_output_dir=True,\n    do_train=True,\n    num_train_epochs=2,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    do_eval=True,\n    fp16=True,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_f1\"\n)","metadata":{"id":"ZJei3t0-wa-8","execution":{"iopub.status.busy":"2022-03-14T14:42:16.112110Z","iopub.execute_input":"2022-03-14T14:42:16.112698Z","iopub.status.idle":"2022-03-14T14:42:16.128417Z","shell.execute_reply.started":"2022-03-14T14:42:16.112664Z","shell.execute_reply":"2022-03-14T14:42:16.127719Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Тренируем","metadata":{"id":"H-IH71LhYN2O"}},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation'],\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n    tokenizer=tokenizer\n)\n\ntrain_results = trainer.train()","metadata":{"id":"u_-A5KAcyjw_","outputId":"9378b490-a92b-43e0-a97b-c964d2545702","execution":{"iopub.status.busy":"2022-03-14T14:42:16.862658Z","iopub.execute_input":"2022-03-14T14:42:16.863226Z","iopub.status.idle":"2022-03-14T14:45:44.285837Z","shell.execute_reply.started":"2022-03-14T14:42:16.863191Z","shell.execute_reply":"2022-03-14T14:45:44.285101Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"Using amp fp16 backend\nThe following columns in the training set  don't have a corresponding argument in `SentimentClassifier.forward` and have been ignored: token_type_ids, text.\n***** Running training *****\n  Num examples = 3000\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 750\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [750/750 03:26, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.503900</td>\n      <td>0.686726</td>\n      <td>0.733000</td>\n      <td>0.780608</td>\n      <td>0.662483</td>\n      <td>0.950000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.358300</td>\n      <td>0.494910</td>\n      <td>0.827000</td>\n      <td>0.834766</td>\n      <td>0.798903</td>\n      <td>0.874000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n  args.max_grad_norm,\nThe following columns in the evaluation set  don't have a corresponding argument in `SentimentClassifier.forward` and have been ignored: token_type_ids, text.\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 8\nSaving model checkpoint to ./results/sentiment-classifier/checkpoint-375\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\ntokenizer config file saved in ./results/sentiment-classifier/checkpoint-375/tokenizer_config.json\nSpecial tokens file saved in ./results/sentiment-classifier/checkpoint-375/special_tokens_map.json\nThe following columns in the evaluation set  don't have a corresponding argument in `SentimentClassifier.forward` and have been ignored: token_type_ids, text.\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 8\nSaving model checkpoint to ./results/sentiment-classifier/checkpoint-750\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\ntokenizer config file saved in ./results/sentiment-classifier/checkpoint-750/tokenizer_config.json\nSpecial tokens file saved in ./results/sentiment-classifier/checkpoint-750/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/sentiment-classifier/checkpoint-750 (score: 0.8347659980897804).\n","output_type":"stream"}]},{"cell_type":"code","source":"pd.DataFrame([train_results.metrics]).T","metadata":{"id":"tLpOqSyZ4RNk","outputId":"37b4e5f5-493c-4bd3-e985-ff2f36106091","execution":{"iopub.status.busy":"2022-03-14T14:45:44.287440Z","iopub.execute_input":"2022-03-14T14:45:44.287832Z","iopub.status.idle":"2022-03-14T14:45:44.317451Z","shell.execute_reply.started":"2022-03-14T14:45:44.287798Z","shell.execute_reply":"2022-03-14T14:45:44.316744Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"                                   0\ntrain_runtime             207.171300\ntrain_samples_per_second   28.962000\ntrain_steps_per_second      3.620000\ntotal_flos                  0.000000\ntrain_loss                  0.491637\nepoch                       2.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>train_runtime</th>\n      <td>207.171300</td>\n    </tr>\n    <tr>\n      <th>train_samples_per_second</th>\n      <td>28.962000</td>\n    </tr>\n    <tr>\n      <th>train_steps_per_second</th>\n      <td>3.620000</td>\n    </tr>\n    <tr>\n      <th>total_flos</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>train_loss</th>\n      <td>0.491637</td>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <td>2.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Посмотрим качество на тесте","metadata":{"id":"iWGXIwLTYQel"}},{"cell_type":"code","source":"test_results = trainer.predict(test_dataset=tokenized_datasets['test'])","metadata":{"id":"o_VLk3Fc4Yhs","outputId":"6a52c827-52f4-4c03-9114-dba5402f450c","execution":{"iopub.status.busy":"2022-03-14T14:45:44.321662Z","iopub.execute_input":"2022-03-14T14:45:44.323700Z","iopub.status.idle":"2022-03-14T14:45:55.788276Z","shell.execute_reply.started":"2022-03-14T14:45:44.323661Z","shell.execute_reply":"2022-03-14T14:45:55.787635Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"The following columns in the test set  don't have a corresponding argument in `SentimentClassifier.forward` and have been ignored: token_type_ids, text.\n***** Running Prediction *****\n  Num examples = 1000\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 00:10]\n    </div>\n    "},"metadata":{}}]},{"cell_type":"code","source":"pd.DataFrame([test_results.metrics]).T","metadata":{"id":"MXkgjipt4cE4","outputId":"d5f26c25-48f0-488c-a8ff-253a7de87bef","execution":{"iopub.status.busy":"2022-03-14T14:45:55.790401Z","iopub.execute_input":"2022-03-14T14:45:55.790664Z","iopub.status.idle":"2022-03-14T14:45:55.801384Z","shell.execute_reply.started":"2022-03-14T14:45:55.790628Z","shell.execute_reply":"2022-03-14T14:45:55.800657Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"                                 0\ntest_loss                 0.419586\ntest_accuracy             0.844000\ntest_f1                   0.848837\ntest_precision            0.823308\ntest_recall               0.876000\ntest_runtime             11.057900\ntest_samples_per_second  90.433000\ntest_steps_per_second    11.304000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>test_loss</th>\n      <td>0.419586</td>\n    </tr>\n    <tr>\n      <th>test_accuracy</th>\n      <td>0.844000</td>\n    </tr>\n    <tr>\n      <th>test_f1</th>\n      <td>0.848837</td>\n    </tr>\n    <tr>\n      <th>test_precision</th>\n      <td>0.823308</td>\n    </tr>\n    <tr>\n      <th>test_recall</th>\n      <td>0.876000</td>\n    </tr>\n    <tr>\n      <th>test_runtime</th>\n      <td>11.057900</td>\n    </tr>\n    <tr>\n      <th>test_samples_per_second</th>\n      <td>90.433000</td>\n    </tr>\n    <tr>\n      <th>test_steps_per_second</th>\n      <td>11.304000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Модель 2: SentimentClassifier with CLS\n\nТот же классификатор, но с добавлением эмбеддинга [CLS] токена с последнего слоя, который мы добавляем путем конкатенации (можно и среднее, конечно, но я рещила что так будет лучше)","metadata":{"id":"OWoPSVFt4kcL"}},{"cell_type":"code","source":"class SentimentClassifierCLS(nn.Module):\n    def __init__(self, n_classes):\n        super().__init__()\n        self.n_classes = n_classes\n        self.bert = AutoModel.from_pretrained(MODEL_NAME)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size*2, n_classes)\n  \n    def forward(self, input_ids, attention_mask, labels=None):\n        last_hidden_state, pooled_output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=False)\n       \n\n        cls = last_hidden_state[:,0,:]\n        stacked_layers = torch.hstack([cls, pooled_output])\n\n        logits = self.out(self.drop(stacked_layers))\n\n        loss = None\n        if labels is not None:\n            loss_fn = nn.CrossEntropyLoss()\n            loss = loss_fn(logits.view(-1, self.n_classes), labels.view(-1))\n\n        output = (logits,)\n        return ((loss,) + output) if loss is not None else output","metadata":{"id":"ntiTygc85RKw","execution":{"iopub.status.busy":"2022-03-14T14:45:55.802564Z","iopub.execute_input":"2022-03-14T14:45:55.803262Z","iopub.status.idle":"2022-03-14T14:45:55.813725Z","shell.execute_reply.started":"2022-03-14T14:45:55.803223Z","shell.execute_reply":"2022-03-14T14:45:55.812987Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"model = SentimentClassifierCLS(num_labels)","metadata":{"id":"SQo3nFln5RKy","outputId":"a9fac560-f5bf-467f-a7d1-640f655e9811","execution":{"iopub.status.busy":"2022-03-14T14:45:55.816135Z","iopub.execute_input":"2022-03-14T14:45:55.816352Z","iopub.status.idle":"2022-03-14T14:45:58.187795Z","shell.execute_reply.started":"2022-03-14T14:45:55.816328Z","shell.execute_reply":"2022-03-14T14:45:58.186971Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"directionality\": \"bidi\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.12.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 119547\n}\n\nloading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\nSome weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of BertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"# заморозим некоторые слои\nfreeze_layers = list(range(5))\nfor layer_id in freeze_layers:\n  for param in list(model.bert.encoder.layer[layer_id].parameters()):\n    param.requires_grad = False","metadata":{"id":"EeuMzlPz5RK0","execution":{"iopub.status.busy":"2022-03-14T14:45:58.189184Z","iopub.execute_input":"2022-03-14T14:45:58.189445Z","iopub.status.idle":"2022-03-14T14:45:58.195937Z","shell.execute_reply.started":"2022-03-14T14:45:58.189408Z","shell.execute_reply":"2022-03-14T14:45:58.194877Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"Зададим параметры обучения с помощью TrainingArguments:","metadata":{"id":"tLm3SA8oYrOj"}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results/sentiment-classifier_cls',\n    overwrite_output_dir=True,\n    do_train=True,\n    num_train_epochs=2,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    do_eval=True,\n    fp16=True,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_f1\"\n)","metadata":{"id":"Vbsyg5u45RK1","outputId":"62c046ef-aa15-4c86-b4b8-9b4a9208d68e","execution":{"iopub.status.busy":"2022-03-14T14:45:58.197115Z","iopub.execute_input":"2022-03-14T14:45:58.197791Z","iopub.status.idle":"2022-03-14T14:45:58.210611Z","shell.execute_reply.started":"2022-03-14T14:45:58.197752Z","shell.execute_reply":"2022-03-14T14:45:58.209840Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Тренируем","metadata":{"id":"sNl5qRkOYxFt"}},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation'],\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n    tokenizer=tokenizer\n)\n\ntrain_results = trainer.train()","metadata":{"id":"COXuUuYt5RK1","outputId":"c9f29d20-5bad-411f-af02-743f2a8db17c","execution":{"iopub.status.busy":"2022-03-14T14:45:58.211931Z","iopub.execute_input":"2022-03-14T14:45:58.212189Z","iopub.status.idle":"2022-03-14T14:49:25.156856Z","shell.execute_reply.started":"2022-03-14T14:45:58.212153Z","shell.execute_reply":"2022-03-14T14:49:25.156054Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"Using amp fp16 backend\nThe following columns in the training set  don't have a corresponding argument in `SentimentClassifierCLS.forward` and have been ignored: token_type_ids, text.\n***** Running training *****\n  Num examples = 3000\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 750\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [750/750 03:26, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.504800</td>\n      <td>0.761136</td>\n      <td>0.688000</td>\n      <td>0.758514</td>\n      <td>0.618687</td>\n      <td>0.980000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.303800</td>\n      <td>0.471862</td>\n      <td>0.830000</td>\n      <td>0.840226</td>\n      <td>0.792553</td>\n      <td>0.894000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n  args.max_grad_norm,\nThe following columns in the evaluation set  don't have a corresponding argument in `SentimentClassifierCLS.forward` and have been ignored: token_type_ids, text.\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 8\nSaving model checkpoint to ./results/sentiment-classifier_cls/checkpoint-375\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\ntokenizer config file saved in ./results/sentiment-classifier_cls/checkpoint-375/tokenizer_config.json\nSpecial tokens file saved in ./results/sentiment-classifier_cls/checkpoint-375/special_tokens_map.json\nThe following columns in the evaluation set  don't have a corresponding argument in `SentimentClassifierCLS.forward` and have been ignored: token_type_ids, text.\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 8\nSaving model checkpoint to ./results/sentiment-classifier_cls/checkpoint-750\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\ntokenizer config file saved in ./results/sentiment-classifier_cls/checkpoint-750/tokenizer_config.json\nSpecial tokens file saved in ./results/sentiment-classifier_cls/checkpoint-750/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/sentiment-classifier_cls/checkpoint-750 (score: 0.8402255639097743).\n","output_type":"stream"}]},{"cell_type":"code","source":"pd.DataFrame([train_results.metrics]).T","metadata":{"id":"Mh1E5Oj35RK2","outputId":"c3990cfe-7808-42f0-a16b-641b2a85a153","execution":{"iopub.status.busy":"2022-03-14T14:49:25.160113Z","iopub.execute_input":"2022-03-14T14:49:25.160442Z","iopub.status.idle":"2022-03-14T14:49:25.173268Z","shell.execute_reply.started":"2022-03-14T14:49:25.160410Z","shell.execute_reply":"2022-03-14T14:49:25.172582Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"                                   0\ntrain_runtime             206.715300\ntrain_samples_per_second   29.025000\ntrain_steps_per_second      3.628000\ntotal_flos                  0.000000\ntrain_loss                  0.492177\nepoch                       2.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>train_runtime</th>\n      <td>206.715300</td>\n    </tr>\n    <tr>\n      <th>train_samples_per_second</th>\n      <td>29.025000</td>\n    </tr>\n    <tr>\n      <th>train_steps_per_second</th>\n      <td>3.628000</td>\n    </tr>\n    <tr>\n      <th>total_flos</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>train_loss</th>\n      <td>0.492177</td>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <td>2.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Посмотрим на качество на тесте","metadata":{"id":"B1V3AqgQYzD3"}},{"cell_type":"code","source":"test_results = trainer.predict(test_dataset=tokenized_datasets['test'])","metadata":{"id":"hwJ7NTdf5RK3","outputId":"d356a040-7d77-44bc-e416-8bda02464021","execution":{"iopub.status.busy":"2022-03-14T14:49:25.174268Z","iopub.execute_input":"2022-03-14T14:49:25.174501Z","iopub.status.idle":"2022-03-14T14:49:36.730961Z","shell.execute_reply.started":"2022-03-14T14:49:25.174468Z","shell.execute_reply":"2022-03-14T14:49:36.730308Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"The following columns in the test set  don't have a corresponding argument in `SentimentClassifierCLS.forward` and have been ignored: token_type_ids, text.\n***** Running Prediction *****\n  Num examples = 1000\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 00:10]\n    </div>\n    "},"metadata":{}}]},{"cell_type":"code","source":"pd.DataFrame([test_results.metrics]).T","metadata":{"id":"9nv-ejjf5RK3","execution":{"iopub.status.busy":"2022-03-14T14:49:36.732075Z","iopub.execute_input":"2022-03-14T14:49:36.732396Z","iopub.status.idle":"2022-03-14T14:49:36.743498Z","shell.execute_reply.started":"2022-03-14T14:49:36.732358Z","shell.execute_reply":"2022-03-14T14:49:36.742572Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"                                 0\ntest_loss                 0.415283\ntest_accuracy             0.849000\ntest_f1                   0.856872\ntest_precision            0.814414\ntest_recall               0.904000\ntest_runtime             10.580100\ntest_samples_per_second  94.517000\ntest_steps_per_second    11.815000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>test_loss</th>\n      <td>0.415283</td>\n    </tr>\n    <tr>\n      <th>test_accuracy</th>\n      <td>0.849000</td>\n    </tr>\n    <tr>\n      <th>test_f1</th>\n      <td>0.856872</td>\n    </tr>\n    <tr>\n      <th>test_precision</th>\n      <td>0.814414</td>\n    </tr>\n    <tr>\n      <th>test_recall</th>\n      <td>0.904000</td>\n    </tr>\n    <tr>\n      <th>test_runtime</th>\n      <td>10.580100</td>\n    </tr>\n    <tr>\n      <th>test_samples_per_second</th>\n      <td>94.517000</td>\n    </tr>\n    <tr>\n      <th>test_steps_per_second</th>\n      <td>11.815000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Модель 3: BertForSequenceClassification\n\nПредобученный BERT для классификации","metadata":{"id":"hM-AAx0ftTMG"}},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)","metadata":{"id":"w9IeoQDzjZj8","execution":{"iopub.status.busy":"2022-03-14T14:49:36.744805Z","iopub.execute_input":"2022-03-14T14:49:36.745055Z","iopub.status.idle":"2022-03-14T14:49:38.999654Z","shell.execute_reply.started":"2022-03-14T14:49:36.745022Z","shell.execute_reply":"2022-03-14T14:49:38.998993Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"directionality\": \"bidi\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.12.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 119547\n}\n\nloading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\nSome weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Зададим параметры обучения с помощью TrainingArguments:","metadata":{"id":"jvHqnZwyY96b"}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results/bert-for-sequence-classification',\n    overwrite_output_dir=True,\n    do_train=True,\n    num_train_epochs=2,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    do_eval=True,\n    fp16=True,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_f1\"\n)","metadata":{"id":"kcuvQ_litcXF","outputId":"08fe0819-6ff8-4fef-a273-428fe0c603e1","execution":{"iopub.status.busy":"2022-03-14T14:49:39.000811Z","iopub.execute_input":"2022-03-14T14:49:39.001050Z","iopub.status.idle":"2022-03-14T14:49:39.016271Z","shell.execute_reply.started":"2022-03-14T14:49:39.001015Z","shell.execute_reply":"2022-03-14T14:49:39.015556Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Тренируем","metadata":{"id":"_bvYMImYY_Xy"}},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation'],\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n    tokenizer=tokenizer\n)\n\ntrain_results = trainer.train()","metadata":{"id":"ZBY0f1jIFMN2","outputId":"963be60b-9fb8-45a7-f7cb-5596bb366207","execution":{"iopub.status.busy":"2022-03-14T14:49:39.017535Z","iopub.execute_input":"2022-03-14T14:49:39.017798Z","iopub.status.idle":"2022-03-14T14:53:34.100813Z","shell.execute_reply.started":"2022-03-14T14:49:39.017764Z","shell.execute_reply":"2022-03-14T14:53:34.099952Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"Using amp fp16 backend\nThe following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n***** Running training *****\n  Num examples = 3000\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 750\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [750/750 03:54, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.488900</td>\n      <td>0.601487</td>\n      <td>0.711000</td>\n      <td>0.766747</td>\n      <td>0.642760</td>\n      <td>0.950000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.341300</td>\n      <td>0.572957</td>\n      <td>0.822000</td>\n      <td>0.829175</td>\n      <td>0.797048</td>\n      <td>0.864000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n  args.max_grad_norm,\nThe following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 8\nSaving model checkpoint to ./results/bert-for-sequence-classification/checkpoint-375\nConfiguration saved in ./results/bert-for-sequence-classification/checkpoint-375/config.json\nModel weights saved in ./results/bert-for-sequence-classification/checkpoint-375/pytorch_model.bin\ntokenizer config file saved in ./results/bert-for-sequence-classification/checkpoint-375/tokenizer_config.json\nSpecial tokens file saved in ./results/bert-for-sequence-classification/checkpoint-375/special_tokens_map.json\nThe following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 8\nSaving model checkpoint to ./results/bert-for-sequence-classification/checkpoint-750\nConfiguration saved in ./results/bert-for-sequence-classification/checkpoint-750/config.json\nModel weights saved in ./results/bert-for-sequence-classification/checkpoint-750/pytorch_model.bin\ntokenizer config file saved in ./results/bert-for-sequence-classification/checkpoint-750/tokenizer_config.json\nSpecial tokens file saved in ./results/bert-for-sequence-classification/checkpoint-750/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/bert-for-sequence-classification/checkpoint-750 (score: 0.8291746641074856).\n","output_type":"stream"}]},{"cell_type":"code","source":"pd.DataFrame([train_results.metrics]).T","metadata":{"id":"TQAqCCIwZcvg","execution":{"iopub.status.busy":"2022-03-14T14:53:34.102230Z","iopub.execute_input":"2022-03-14T14:53:34.103048Z","iopub.status.idle":"2022-03-14T14:53:34.115093Z","shell.execute_reply.started":"2022-03-14T14:53:34.103000Z","shell.execute_reply":"2022-03-14T14:53:34.114335Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"                                     0\ntrain_runtime             2.348511e+02\ntrain_samples_per_second  2.554800e+01\ntrain_steps_per_second    3.194000e+00\ntotal_flos                7.893332e+14\ntrain_loss                5.226143e-01\nepoch                     2.000000e+00","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>train_runtime</th>\n      <td>2.348511e+02</td>\n    </tr>\n    <tr>\n      <th>train_samples_per_second</th>\n      <td>2.554800e+01</td>\n    </tr>\n    <tr>\n      <th>train_steps_per_second</th>\n      <td>3.194000e+00</td>\n    </tr>\n    <tr>\n      <th>total_flos</th>\n      <td>7.893332e+14</td>\n    </tr>\n    <tr>\n      <th>train_loss</th>\n      <td>5.226143e-01</td>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <td>2.000000e+00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Смотрим результаты на тесте","metadata":{"id":"Ty7P8Ar8ZCPg"}},{"cell_type":"code","source":"test_results = trainer.predict(test_dataset=tokenized_datasets['test'])","metadata":{"id":"kWG1M6yS8cEG","execution":{"iopub.status.busy":"2022-03-14T14:53:34.116747Z","iopub.execute_input":"2022-03-14T14:53:34.117259Z","iopub.status.idle":"2022-03-14T14:53:46.059002Z","shell.execute_reply.started":"2022-03-14T14:53:34.117218Z","shell.execute_reply":"2022-03-14T14:53:46.058263Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n***** Running Prediction *****\n  Num examples = 1000\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 00:10]\n    </div>\n    "},"metadata":{}}]},{"cell_type":"code","source":"pd.DataFrame([test_results.metrics]).T","metadata":{"id":"83uFlneqKSmK","execution":{"iopub.status.busy":"2022-03-14T14:53:46.060121Z","iopub.execute_input":"2022-03-14T14:53:46.060875Z","iopub.status.idle":"2022-03-14T14:53:46.072327Z","shell.execute_reply.started":"2022-03-14T14:53:46.060837Z","shell.execute_reply":"2022-03-14T14:53:46.071326Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"                                 0\ntest_loss                 0.553569\ntest_accuracy             0.815000\ntest_f1                   0.820563\ntest_precision            0.796610\ntest_recall               0.846000\ntest_runtime             10.705300\ntest_samples_per_second  93.411000\ntest_steps_per_second    11.676000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>test_loss</th>\n      <td>0.553569</td>\n    </tr>\n    <tr>\n      <th>test_accuracy</th>\n      <td>0.815000</td>\n    </tr>\n    <tr>\n      <th>test_f1</th>\n      <td>0.820563</td>\n    </tr>\n    <tr>\n      <th>test_precision</th>\n      <td>0.796610</td>\n    </tr>\n    <tr>\n      <th>test_recall</th>\n      <td>0.846000</td>\n    </tr>\n    <tr>\n      <th>test_runtime</th>\n      <td>10.705300</td>\n    </tr>\n    <tr>\n      <th>test_samples_per_second</th>\n      <td>93.411000</td>\n    </tr>\n    <tr>\n      <th>test_steps_per_second</th>\n      <td>11.676000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## *Модель 4: SentimentClassifier with CLS tokens from all layers\n\nНаш классификатор как модель 2, только берем [CLS] токен не с посленего слоя, а среднее по всем слоям ","metadata":{"id":"GO-19_l5C6Xe"}},{"cell_type":"code","source":"class SentimentClassifierPooledCLS(nn.Module):\n    def __init__(self, n_classes):\n        super().__init__()\n        self.n_classes = n_classes\n        self.bert = AutoModel.from_pretrained(MODEL_NAME)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size*2, n_classes)\n  \n    def forward(self, input_ids, attention_mask, labels=None):\n        _, pooled_output, hidden_states = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=False,\n            output_hidden_states=True)\n       \n        hidden_states = torch.stack(hidden_states)\n        hidden_cls = hidden_states[:,:,1,:]\n        hidden_cls = hidden_cls.mean(axis=0)\n\n        stacked_layers = torch.hstack([hidden_cls, pooled_output])\n\n        logits = self.out(self.drop(stacked_layers))\n\n        loss = None\n        if labels is not None:\n            loss_fn = nn.CrossEntropyLoss()\n            loss = loss_fn(logits.view(-1, self.n_classes), labels.view(-1))\n\n        output = (logits,)\n        return ((loss,) + output) if loss is not None else output","metadata":{"id":"YQOB-DMjDTKl","execution":{"iopub.status.busy":"2022-03-14T14:53:46.073865Z","iopub.execute_input":"2022-03-14T14:53:46.074185Z","iopub.status.idle":"2022-03-14T14:53:46.085048Z","shell.execute_reply.started":"2022-03-14T14:53:46.074151Z","shell.execute_reply":"2022-03-14T14:53:46.083891Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"model = SentimentClassifierPooledCLS(num_labels)","metadata":{"id":"bKDnrtxyDTKn","outputId":"68ffc80f-d5cc-4f59-e3f3-393b730404a5","execution":{"iopub.status.busy":"2022-03-14T14:53:46.086712Z","iopub.execute_input":"2022-03-14T14:53:46.087103Z","iopub.status.idle":"2022-03-14T14:53:48.431924Z","shell.execute_reply.started":"2022-03-14T14:53:46.087039Z","shell.execute_reply":"2022-03-14T14:53:48.431204Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"directionality\": \"bidi\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.12.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 119547\n}\n\nloading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\nSome weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of BertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"# заморозим некоторые слои\nfreeze_layers = list(range(5))\nfor layer_id in freeze_layers:\n    for param in list(model.bert.encoder.layer[layer_id].parameters()):\n        param.requires_grad = False","metadata":{"id":"kglIy3ciDTKq","execution":{"iopub.status.busy":"2022-03-14T14:53:48.433301Z","iopub.execute_input":"2022-03-14T14:53:48.433558Z","iopub.status.idle":"2022-03-14T14:53:48.439234Z","shell.execute_reply.started":"2022-03-14T14:53:48.433522Z","shell.execute_reply":"2022-03-14T14:53:48.438417Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"Зададим параметры обучения:","metadata":{"id":"dW25M-b6ZN41"}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results/sentiment-classifier_pooled_cls',\n    overwrite_output_dir=True,\n    do_train=True,\n    num_train_epochs=2,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    do_eval=True,\n    fp16=True,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_f1\"\n)","metadata":{"id":"m1ckbzYkDTKr","execution":{"iopub.status.busy":"2022-03-14T14:53:48.440530Z","iopub.execute_input":"2022-03-14T14:53:48.440799Z","iopub.status.idle":"2022-03-14T14:53:49.332057Z","shell.execute_reply.started":"2022-03-14T14:53:48.440765Z","shell.execute_reply":"2022-03-14T14:53:49.331249Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Тренируем","metadata":{"id":"P4313skAZMVu"}},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation'],\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n    tokenizer=tokenizer\n)\n\ntrain_results = trainer.train()","metadata":{"id":"bcezOfpjDTKr","outputId":"70ae26f4-6b30-42a1-faa2-76e1cd76a9d7","execution":{"iopub.status.busy":"2022-03-14T14:53:49.333800Z","iopub.execute_input":"2022-03-14T14:53:49.334082Z","iopub.status.idle":"2022-03-14T14:57:17.953741Z","shell.execute_reply.started":"2022-03-14T14:53:49.334043Z","shell.execute_reply":"2022-03-14T14:57:17.952984Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stderr","text":"Using amp fp16 backend\nThe following columns in the training set  don't have a corresponding argument in `SentimentClassifierPooledCLS.forward` and have been ignored: token_type_ids, text.\n***** Running training *****\n  Num examples = 3000\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 750\n/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n  args.max_grad_norm,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [750/750 03:28, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.508900</td>\n      <td>0.617051</td>\n      <td>0.722000</td>\n      <td>0.770248</td>\n      <td>0.656338</td>\n      <td>0.932000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.385900</td>\n      <td>0.532918</td>\n      <td>0.811000</td>\n      <td>0.820171</td>\n      <td>0.782214</td>\n      <td>0.862000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set  don't have a corresponding argument in `SentimentClassifierPooledCLS.forward` and have been ignored: token_type_ids, text.\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 8\nSaving model checkpoint to ./results/sentiment-classifier_pooled_cls/checkpoint-375\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\ntokenizer config file saved in ./results/sentiment-classifier_pooled_cls/checkpoint-375/tokenizer_config.json\nSpecial tokens file saved in ./results/sentiment-classifier_pooled_cls/checkpoint-375/special_tokens_map.json\nThe following columns in the evaluation set  don't have a corresponding argument in `SentimentClassifierPooledCLS.forward` and have been ignored: token_type_ids, text.\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 8\nSaving model checkpoint to ./results/sentiment-classifier_pooled_cls/checkpoint-750\nTrainer.model is not a `PreTrainedModel`, only saving its state dict.\ntokenizer config file saved in ./results/sentiment-classifier_pooled_cls/checkpoint-750/tokenizer_config.json\nSpecial tokens file saved in ./results/sentiment-classifier_pooled_cls/checkpoint-750/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/sentiment-classifier_pooled_cls/checkpoint-750 (score: 0.8201712654614652).\n","output_type":"stream"}]},{"cell_type":"code","source":"pd.DataFrame([train_results.metrics]).T","metadata":{"id":"AtMEuFywDTKs","execution":{"iopub.status.busy":"2022-03-14T14:57:17.955204Z","iopub.execute_input":"2022-03-14T14:57:17.956037Z","iopub.status.idle":"2022-03-14T14:57:17.966946Z","shell.execute_reply.started":"2022-03-14T14:57:17.955991Z","shell.execute_reply":"2022-03-14T14:57:17.966115Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"                                   0\ntrain_runtime             208.382200\ntrain_samples_per_second   28.793000\ntrain_steps_per_second      3.599000\ntotal_flos                  0.000000\ntrain_loss                  0.516331\nepoch                       2.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>train_runtime</th>\n      <td>208.382200</td>\n    </tr>\n    <tr>\n      <th>train_samples_per_second</th>\n      <td>28.793000</td>\n    </tr>\n    <tr>\n      <th>train_steps_per_second</th>\n      <td>3.599000</td>\n    </tr>\n    <tr>\n      <th>total_flos</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>train_loss</th>\n      <td>0.516331</td>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <td>2.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Смотрим результаты на тесте","metadata":{"id":"4kt9zaA5ZKQh"}},{"cell_type":"code","source":"test_results = trainer.predict(test_dataset=tokenized_datasets['test'])","metadata":{"id":"f4epq7pDDTKs","execution":{"iopub.status.busy":"2022-03-14T14:57:17.968056Z","iopub.execute_input":"2022-03-14T14:57:17.968385Z","iopub.status.idle":"2022-03-14T14:57:29.582358Z","shell.execute_reply.started":"2022-03-14T14:57:17.968347Z","shell.execute_reply":"2022-03-14T14:57:29.581710Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stderr","text":"The following columns in the test set  don't have a corresponding argument in `SentimentClassifierPooledCLS.forward` and have been ignored: token_type_ids, text.\n***** Running Prediction *****\n  Num examples = 1000\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 00:10]\n    </div>\n    "},"metadata":{}}]},{"cell_type":"code","source":"pd.DataFrame([test_results.metrics]).T","metadata":{"id":"17sy30xRDTKt","execution":{"iopub.status.busy":"2022-03-14T14:57:29.583714Z","iopub.execute_input":"2022-03-14T14:57:29.584118Z","iopub.status.idle":"2022-03-14T14:57:29.593855Z","shell.execute_reply.started":"2022-03-14T14:57:29.584080Z","shell.execute_reply":"2022-03-14T14:57:29.592910Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"                                 0\ntest_loss                 0.440089\ntest_accuracy             0.822000\ntest_f1                   0.832075\ntest_precision            0.787500\ntest_recall               0.882000\ntest_runtime             10.576700\ntest_samples_per_second  94.547000\ntest_steps_per_second    11.818000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>test_loss</th>\n      <td>0.440089</td>\n    </tr>\n    <tr>\n      <th>test_accuracy</th>\n      <td>0.822000</td>\n    </tr>\n    <tr>\n      <th>test_f1</th>\n      <td>0.832075</td>\n    </tr>\n    <tr>\n      <th>test_precision</th>\n      <td>0.787500</td>\n    </tr>\n    <tr>\n      <th>test_recall</th>\n      <td>0.882000</td>\n    </tr>\n    <tr>\n      <th>test_runtime</th>\n      <td>10.576700</td>\n    </tr>\n    <tr>\n      <th>test_samples_per_second</th>\n      <td>94.547000</td>\n    </tr>\n    <tr>\n      <th>test_steps_per_second</th>\n      <td>11.818000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Итоги?\nКак видим, в целом, все варианты моделей добиваются высокого качества на данных.\n\nЛучше всего себя показала модель `SentimentClassifierCLS`, которая использует конкатенацию эмбеддинга CLS токена с выходом из пулинга, что говорит о том, что CLS токен сам по себе аггрегирует полезную для классификации информацию. Интересно, что модель, использующая эмбеддинги CLS токена со всех слоев выдает качество хуже. Потенциально на других слоях CLS токен аггрегирует информацию не о (скажем так) семантике, которая важна дя определения тональности, а чем-то еще (ср. работы по пробингу, которые говорят о том что модель собирает поверхностную информацию на начальных слоях, синтаксическую на средних и семантическую на верхних), отсюда слои скорее сбивают классификатор. ","metadata":{"id":"oQWFIrEfaYLE"}},{"cell_type":"markdown","source":"## 5. Тестируем модель на отзывах GooglePlay\nЧтобы не заморачиваться с загрузкой модели, давайте посмотрим на последнюю","metadata":{"id":"vUrgxl1HO51I"}},{"cell_type":"code","source":"!pip install gdown","metadata":{"execution":{"iopub.status.busy":"2022-03-14T15:17:42.355345Z","iopub.execute_input":"2022-03-14T15:17:42.355614Z","iopub.status.idle":"2022-03-14T15:18:05.141296Z","shell.execute_reply.started":"2022-03-14T15:17:42.355584Z","shell.execute_reply":"2022-03-14T15:18:05.140405Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-4.4.0.tar.gz (14 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.62.3)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from gdown) (4.10.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.16.0)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.7/site-packages (from gdown) (2.26.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.4.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown) (2.3.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.26.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.1)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2.0.9)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\nBuilding wheels for collected packages: gdown\n  Building wheel for gdown (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14775 sha256=3f8771858133a96b0aacfeab0d30c57620b16a6ef93e629f845ba0770aec5b28\n  Stored in directory: /root/.cache/pip/wheels/fb/c3/0e/c4d8ff8bfcb0461afff199471449f642179b74968c15b7a69c\nSuccessfully built gdown\nInstalling collected packages: gdown\nSuccessfully installed gdown-4.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown --id 1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv","metadata":{"id":"oqMyCYKZyxcT","execution":{"iopub.status.busy":"2022-03-14T15:18:05.145166Z","iopub.execute_input":"2022-03-14T15:18:05.145428Z","iopub.status.idle":"2022-03-14T15:18:10.130626Z","shell.execute_reply.started":"2022-03-14T15:18:05.145402Z","shell.execute_reply":"2022-03-14T15:18:10.129776Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.7/site-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv\nTo: /kaggle/working/reviews.csv\n100%|███████████████████████████████████████| 7.17M/7.17M [00:00<00:00, 212MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv(\"reviews.csv\")","metadata":{"id":"463LDMOkPOF0","execution":{"iopub.status.busy":"2022-03-14T15:18:10.134128Z","iopub.execute_input":"2022-03-14T15:18:10.134363Z","iopub.status.idle":"2022-03-14T15:18:10.256788Z","shell.execute_reply.started":"2022-03-14T15:18:10.134336Z","shell.execute_reply":"2022-03-14T15:18:10.256014Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"def to_sentiment(rating):\n    rating = int(rating)\n    if rating <= 2:\n        return 0\n    elif rating == 3:\n        return 1\n    else: \n        return 2\n\ndf['sentiment'] = df.score.apply(to_sentiment)","metadata":{"id":"KTnradW5PTye","execution":{"iopub.status.busy":"2022-03-14T15:18:10.259079Z","iopub.execute_input":"2022-03-14T15:18:10.259344Z","iopub.status.idle":"2022-03-14T15:18:10.277242Z","shell.execute_reply.started":"2022-03-14T15:18:10.259307Z","shell.execute_reply":"2022-03-14T15:18:10.276116Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"class_names = ['negative', 'neutral', 'positive']","metadata":{"id":"Wi3uFm4mPYR7","execution":{"iopub.status.busy":"2022-03-14T15:18:10.279190Z","iopub.execute_input":"2022-03-14T15:18:10.279547Z","iopub.status.idle":"2022-03-14T15:18:10.286147Z","shell.execute_reply.started":"2022-03-14T15:18:10.279510Z","shell.execute_reply":"2022-03-14T15:18:10.285386Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"Рандомно выберем по одному отзыву на класс","metadata":{"id":"VxaL0inUZlFE"}},{"cell_type":"code","source":"neg = df[df.sentiment==0].sample(1)['content'].to_list()\nneut = df[df.sentiment==1].sample(1)['content'].to_list()\npos = df[df.sentiment==2].sample(1)['content'].to_list()","metadata":{"id":"9FSdVwmsPY8k","execution":{"iopub.status.busy":"2022-03-14T15:18:10.287836Z","iopub.execute_input":"2022-03-14T15:18:10.288142Z","iopub.status.idle":"2022-03-14T15:18:10.306241Z","shell.execute_reply.started":"2022-03-14T15:18:10.288084Z","shell.execute_reply":"2022-03-14T15:18:10.305510Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"Берем модельку","metadata":{"id":"IO_OXeSPZoXw"}},{"cell_type":"code","source":"model = trainer.model.to('cpu')","metadata":{"id":"i-UTnj6fR6UD","execution":{"iopub.status.busy":"2022-03-14T15:18:10.307663Z","iopub.execute_input":"2022-03-14T15:18:10.307958Z","iopub.status.idle":"2022-03-14T15:18:11.252212Z","shell.execute_reply.started":"2022-03-14T15:18:10.307921Z","shell.execute_reply":"2022-03-14T15:18:11.251461Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"Проверяем:","metadata":{"id":"qOJBdT_kZp0p"}},{"cell_type":"code","source":"tokenized = tokenizer(neg, return_tensors='pt')\nlogits = model(tokenized['input_ids'], tokenized['attention_mask'])[0]\npred = torch.argmax(F.softmax(logits, dim=1))\nprint('Text:', neg[0])\nprint(f'True label:    0\\nPred label:    {pred}')","metadata":{"id":"3LeY-ol2R_UW","execution":{"iopub.status.busy":"2022-03-14T15:22:11.705800Z","iopub.execute_input":"2022-03-14T15:22:11.706259Z","iopub.status.idle":"2022-03-14T15:22:11.819174Z","shell.execute_reply.started":"2022-03-14T15:22:11.706225Z","shell.execute_reply":"2022-03-14T15:22:11.818415Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"Text: Not a movable app\nTrue label:    0\nPred label:    0\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenized = tokenizer(neut, return_tensors='pt')\nlogits = model(tokenized['input_ids'], tokenized['attention_mask'])[0]\npred = torch.argmax(F.softmax(logits, dim=1))\nprint('Text:', neut[0])\nprint(f'True label:    1\\nPred label:    {pred}')","metadata":{"id":"idGM3UIhSPqh","execution":{"iopub.status.busy":"2022-03-14T15:22:12.637767Z","iopub.execute_input":"2022-03-14T15:22:12.638633Z","iopub.status.idle":"2022-03-14T15:22:12.721788Z","shell.execute_reply.started":"2022-03-14T15:22:12.638594Z","shell.execute_reply":"2022-03-14T15:22:12.720869Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"Text: It alright\nTrue label:    1\nPred label:    0\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenized = tokenizer(pos, return_tensors='pt')\nlogits = model(tokenized['input_ids'], tokenized['attention_mask'])[0]\npred = torch.argmax(F.softmax(logits, dim=1))\nprint('Text:', pos[0])\nprint(f'True label:    2\\nPred label:    {pred}')","metadata":{"id":"y3QJQH-wSQeZ","execution":{"iopub.status.busy":"2022-03-14T15:22:13.563204Z","iopub.execute_input":"2022-03-14T15:22:13.563467Z","iopub.status.idle":"2022-03-14T15:22:13.692134Z","shell.execute_reply.started":"2022-03-14T15:22:13.563435Z","shell.execute_reply":"2022-03-14T15:22:13.691267Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"Text: This is a great organizational tool. It literally combines all my relevant tasks in one place.\nTrue label:    2\nPred label:    1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**NB:** модель тренировалась на 2 класса, а данные гугла рассчитаны на 3, так что не совсем правильно использовать эту модель для этих данных, но раз уж такое задание\n\nВ остальном видим, что модель хорошо справдяется и на этих данных:\n\n- дает отрицательную полярность для негативного отзыва\n- дает положительную полярность для положительного отзыва\n- дает положительную полярность для нейтрального отзыва, с положительной оценкой","metadata":{"id":"lfV3FKoCZs5C"}}]}